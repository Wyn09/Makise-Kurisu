from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig
import torch
from peft import PeftModel
from baidu_translate import translate
"""
ä½¿ç”¨å°è£…çš„ChatModelç±»
"""

class ChatModel:
    def __init__(self, 
        base_model=r"pretrained_models\Qwen\Qwen2.5-0.5B-Instruct",    # åŸºç¡€æ¨¡å‹è·¯å¾„
        lora_path=r"models weights\chat_model\Qwen2.5-0.5B-Instruct\lora\train_2025-03-10-15-22-21",     # LoRAæƒé‡è·¯å¾„
        quantization=None, 
        system_prompt="",
        temperature=1.0,
        top_k=20,
        top_p=0.8,
        max_new_tokens=128,
        repetition_penalty=1.2,
        role="kurisu"
    ):
        self.language = "ä¸­æ–‡"
        self.system_prompt = system_prompt
        self.role = role
        self.sys_prompt_dic = {
            "kurisu": {
                "ä¸­æ–‡": r"./system_prompts/Kurisu_sys_prompt_ZH.txt",
                "ç²¤è¯­": r"./system_prompts/Kurisu_sys_prompt_ZH.txt",
                "è‹±æ–‡": r"./system_prompts/Kurisu_sys_prompt_EN.txt",
                "æ—¥æ–‡": r"./system_prompts/Kurisu_sys_prompt_JP.txt",
                "ä¸­è‹±æ··åˆ": r"./system_prompts/Kurisu_sys_prompt_ZH.txt",
                "æ—¥è‹±æ··åˆ": r"./system_prompts/Kurisu_sys_prompt_JP.txt",
                "å¤šè¯­ç§æ··åˆ": r"./system_prompts/Kurisu_sys_prompt_ZH.txt"
            },
            "2b":{
                "ä¸­æ–‡": r"./system_prompts/2B_sys_prompt_ZH.txt",
                "ç²¤è¯­": r"./system_prompts/2B_sys_prompt_ZH.txt",
                "è‹±æ–‡": r"./system_prompts/2B_sys_prompt_EN.txt",
                "æ—¥æ–‡": r"./system_prompts/2B_sys_prompt_JP.txt",
                "ä¸­è‹±æ··åˆ": r"./system_prompts/Kurisu_sys_prompt_ZH.txt",
                "æ—¥è‹±æ··åˆ": r"./system_prompts/Kurisu_sys_prompt_JP.txt",
                "å¤šè¯­ç§æ··åˆ": r"./system_prompts/Kurisu_sys_prompt_ZH.txt"
            }
        }
        
        if quantization == "4bit":
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quantization_type="nf4",  # ä½¿ç”¨NormalFloat4é‡åŒ–
                bnb_4bit_use_double_quantization=True,  # å¯ç”¨åŒé‡é‡åŒ–èŠ‚çœæ˜¾å­˜
                bnb_4bit_compute_dtype=torch.bfloat16
            )
        elif quantization == "8bit":
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True
            )

        # åˆå§‹åŒ–åŸºç¡€æ¨¡å‹ï¼ˆä¿®æ”¹æ¨¡å‹åŠ è½½æ–¹å¼ï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model,
            quantization_config=quantization_config if quantization is not None else None,  # åº”ç”¨é‡åŒ–é…ç½®
            device_map="auto",
            torch_dtype=torch.bfloat16,
            # token=True  # è‹¥ä½¿ç”¨ç§æœ‰æ¨¡å‹éœ€éªŒè¯
        )

        if lora_path is not None:
            # # åº”ç”¨LoRAé€‚é…å™¨
            self.model = PeftModel.from_pretrained(self.model, lora_path)
            self.model = self.model.merge_and_unload()  # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹

        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        # ç”Ÿæˆå‚æ•°é…ç½®ï¼ˆå¯¹åº”åŸSamplingParamsï¼‰
        self.generation_config = GenerationConfig(
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            max_new_tokens=max_new_tokens,
            repetition_penalty=repetition_penalty,
            pad_token_id=self.tokenizer.eos_token_id,
            do_sample=True
        )
        
    def chat(self, history = []):
            query = input("\nğŸ¤— >>: ")
            if query.lower() == "exit":
                return history, "exit"
            if self.language not in ["ä¸­æ–‡", "ç²¤è¯­"]:
                query = translate(query, self.language)
                history, answer = self.chat_with_history(query, history, True)
            else:
                history, answer = self.chat_with_history(query, history, True)
            return history, answer
    def chat_with_history(self, query, history=[], through_chat=False):
        if self.language not in ["ä¸­æ–‡", "ç²¤è¯­"] and not through_chat:
            query = translate(query, self.language)
        def build_multiturn_prompt(history, query):
            """æ„å»ºå«å†å²å¯¹è¯çš„promptï¼ˆä¿æŒåŸæ ¼å¼ï¼‰"""
            prompt = f"<|im_start|>system\n{self.system_prompt}<|im_end|>\n"
            for user_msg, assistant_msg in history:
                prompt += f"<|im_start|>user\n{user_msg}<|im_end|>\n"
                prompt += f"<|im_start|>assistant\n{assistant_msg}<|im_end|>\n"
            prompt += f"<|im_start|>user\n{query}<|im_end|>\n"
            prompt += "<|im_start|>assistant\n"
            return prompt

        # æ„å»ºpromptå¹¶ç¼–ç 
        prompt = build_multiturn_prompt(history, query)
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        # ç”Ÿæˆå“åº”
        outputs = self.model.generate(
            **inputs,
            generation_config=self.generation_config,
        )
        
        # è§£ç å¹¶æå–æ–°ç”Ÿæˆçš„æ–‡æœ¬
        answer = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).replace("\n\n", "\n")
        # å› ä¸ºæœ‰thinkï¼Œä¸èƒ½æŠŠthinkå¡«å…¥å†å²
        history.append((query, answer.strip()))
        return history, answer

    def set_model_language(self, language="ä¸­æ–‡"):
        self.language = language
        if self.language not in ["ä¸­æ–‡", "ç²¤è¯­"]:
                self.system_prompt = translate(self.system_prompt, self.language)
        # è¯»å–è§’è‰²ä¿¡æ¯
        with open(self.sys_prompt_dic[self.role][language], "r", encoding="utf-8") as f:
            self.system_prompt += "".join(f.readlines())


if __name__ == "__main__":
    """
    å¯¹å¤–è°ƒç”¨ç¤ºä¾‹
    """
    system_prompt = "æ ¹æ®ç”¨æˆ·æ­£åœ¨åšçš„äº‹æƒ…ï¼Œä½ éœ€è¦æ ¹æ®æä¾›çš„ä¿¡æ¯ä»¥ç¬¬ä¸€äººç§°å¯¹ç”¨æˆ·è¿›è¡Œè°ƒä¾ƒï¼Œä¸è¦è¾“å‡ºè®²è¯äººç§°å‘¼ã€‚å¯¹è¯è¦ç¬¦åˆè§’è‰²æ€§æ ¼ã€‚"
    base_model = r"E:\VsCode-Python\pretrained_models\Qwen\Qwen2.5-3B-Instruct"
    lora_path = None
    quantization = "4bit"
    model = ChatModel(
        base_model=base_model, 
        lora_path=lora_path, 
        quantization=quantization, 
        system_prompt=system_prompt
    )
    history = []
    while 1:
        model.set_model_language("æ—¥æ–‡")
        history, text = model.chat(history)
        if text.lower() == "exit":
            break
        print(text)
    print("history: \n", history)