{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 基础模型路径\n",
    "base_model = \"./pretrained_models/Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# LoRA权重路径\n",
    "lora_path = \"./models weights/chat_model/Qwen2.5-0.5B-Instruct/lora/train_2025-03-10-15-22-21\"\n",
    "\n",
    "# 初始化模型（启用LoRA支持）\n",
    "llm = LLM(\n",
    "    model=base_model,\n",
    "    enable_lora=True,\n",
    "    max_lora_rank=64,    # ✅ 唯一需要设置 max_lora_rank 的地方\n",
    "    max_loras=2,        # 允许同时加载的LoRA数量\n",
    "    max_cpu_loras=2,    # CPU内存中保留的LoRA数量\n",
    "    tensor_parallel_size=1,\n",
    "    # gpu_memory_utilization=0.8,\n",
    "    # enforce_eager=True,\n",
    "    # max_model_len=4096,\n",
    "    # quantization=\"awq\",\n",
    ")\n",
    "\n",
    "# 定义LoRA请求\n",
    "lora_request = LoRARequest(\n",
    "    lora_name=\"kurisu_lora\",  # 自定义名称，用于区分不同LoRA\n",
    "    lora_local_path=lora_path,\n",
    "    lora_int_id = 1,\n",
    ")\n",
    "\n",
    "# 生成参数配置\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    max_tokens=512,\n",
    "    frequency_penalty=1.2,  # 抑制重复\n",
    "    stop=[\"\\n###\", \"</end>\"], # 自定义停止标记\n",
    "    # max_tokens=512,              # 减少生成长度\n",
    "    # skip_special_tokens=True,     # 避免特殊token处理开销\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./character info/角色介绍.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    intro = f.readlines()\n",
    "with open(\"./character info/角色设定.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chara = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def chat_with_history(query, sampling_params, lora_request, introduction=\"\", chara=\"\", history=[]):\n",
    "\n",
    "    def build_multiturn_prompt(system_prompt: str, history: list[tuple[str, str]], query: str) -> str:\n",
    "        \"\"\"构建含历史对话的prompt\"\"\"\n",
    "        prompt = f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n",
    "        for user_msg, assistant_msg in history:\n",
    "            prompt += f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
    "            prompt += f\"<|im_start|>assistant\\n{assistant_msg}<|im_end|>\\n\"\n",
    "        prompt += f\"<|im_start|>user\\n{query}<|im_end|>\\n\"\n",
    "        prompt += \"<|im_start|>assistant\\n\"\n",
    "        return prompt\n",
    "    \n",
    "    # 使用示例\n",
    "    # system_prompt = f\"\"\"你是牧濑红莉栖，扮演傲娇毒舌但内心温柔的天才少女，偶尔流露出笨拙的一面。根据角色介绍模拟，以下是该角色介绍:{introduction}。\"\"\"\n",
    "    system_prompt = f\"\"\"你输出的文本只能是台词，不能有其他描述。角色介绍:{introduction}。角色设定:{chara}\n",
    "\"\"\"\n",
    "    \n",
    "    prompt = build_multiturn_prompt(system_prompt, history, query)\n",
    "    outputs = llm.generate(\n",
    "        prompt,\n",
    "        sampling_params,\n",
    "        lora_request=lora_request  # 关键：传入LoRA请求\n",
    "    )\n",
    "    # 输出结果\n",
    "    answer = outputs[0].outputs[0].text\n",
    "    history.append([query, answer])\n",
    "    return history, answer\n",
    "\n",
    "def chat(sampling_params, lora_request):\n",
    "    history = []\n",
    "    with open(\"./角色介绍.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        introduction = f.readlines()\n",
    "    while True:\n",
    "        query = input(\"请输入 (exit退出): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        history, answer = chat_with_history(query, sampling_params, lora_request, introduction, chara, history)\n",
    "        print(answer)\n",
    "        time.sleep(1)\n",
    "    return answer\n",
    "chat(sampling_params, lora_request)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
