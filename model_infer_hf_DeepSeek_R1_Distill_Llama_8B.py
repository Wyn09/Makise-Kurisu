from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig
import torch
from peft import PeftModel
from baidu_translate import translate
"""
ä½¿ç”¨å°è£…çš„ChatModelç±»
"""

class ChatModel:
    def __init__(self):
        self.language = "ä¸­æ–‡"
        self.system_prompt = ""
        self.sys_prompt_dic = {
            "ä¸­æ–‡": r"./system_prompts/Kurisu_sys_prompt_ZH.txt",
            "ç²¤è¯­": r"./system_prompts/Kurisu_sys_prompt_ZH.txt",
            "è‹±æ–‡": r"./system_prompts/Kurisu_sys_prompt_EN.txt",
            "æ—¥æ–‡": r"./system_prompts/Kurisu_sys_prompt_JP.txt"
        }

        # åŸºç¡€æ¨¡å‹è·¯å¾„
        base_model = r"pretrained_models\DeepSeek-R1-Distill-Llama-8B"
        # LoRAæƒé‡è·¯å¾„
        lora_path = r"models weights\chat_model\DeepSeek-R1-8B-Distill\lora\train_2025-03-13-08-58-23"
        
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",  # ä½¿ç”¨NormalFloat4é‡åŒ–
            bnb_4bit_use_double_quant=True,  # å¯ç”¨åŒé‡é‡åŒ–èŠ‚çœæ˜¾å­˜
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        # åˆå§‹åŒ–åŸºç¡€æ¨¡å‹ï¼ˆä¿®æ”¹æ¨¡å‹åŠ è½½æ–¹å¼ï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model,
            quantization_config=quant_config,  # åº”ç”¨4-bité‡åŒ–é…ç½®
            device_map="auto",
            torch_dtype=torch.bfloat16,
            # token=True  # è‹¥ä½¿ç”¨ç§æœ‰æ¨¡å‹éœ€éªŒè¯
        )

        # # åº”ç”¨LoRAé€‚é…å™¨
        self.model = PeftModel.from_pretrained(self.model, lora_path)
        self.model = self.model.merge_and_unload()  # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹

        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        # ç”Ÿæˆå‚æ•°é…ç½®ï¼ˆå¯¹åº”åŸSamplingParamsï¼‰
        self.generation_config = GenerationConfig(
            temperature=1.2,
            top_k=20,
            top_p=0.8,
            max_new_tokens=1024,
            repetition_penalty=1.2,
            pad_token_id=self.tokenizer.eos_token_id,
            do_sample=True
        )
        
    def chat(self, history = []):
            query = input("\nğŸ¤— >>: ")
            if query.lower() == "exit":
                return history, "exit"
            if self.language not in ["ä¸­æ–‡", "ç²¤è¯­"]:
                query = translate(query, self.language)
                history, answer = self.chat_with_history(query, history)
            else:
                history, answer = self.chat_with_history(query, history)
            return history, answer
    def chat_with_history(self, query, history=[]):
        def build_multiturn_prompt( history, query):
            # ç³»ç»Ÿæç¤ºä»…å‡ºç°åœ¨é¦–è½®å¯¹è¯å‰
            prompt = f"<ï½œbeginâ–ofâ–sentenceï½œ>{self.system_prompt}"
            # å†å²å¯¹è¯å¤„ç†
            for idx, (user_msg, assistant_msg) in enumerate(history):
                prompt += f"<ï½œUserï½œ>{user_msg}"
                prompt += f"<ï½œAssistantï½œ>{assistant_msg}<ï½œendâ–ofâ–sentenceï½œ>"
            prompt += f"<ï½œUserï½œ>{query}"  
            prompt += "<ï½œAssistantï½œ><think>"  
            return prompt

        # æ„å»ºpromptå¹¶ç¼–ç 
        prompt = build_multiturn_prompt(history, query)
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        # ç”Ÿæˆå“åº”
        outputs = self.model.generate(
            **inputs,
            generation_config=self.generation_config,
        )
        
        # è§£ç å¹¶æå–æ–°ç”Ÿæˆçš„æ–‡æœ¬
        answer = "<think>" + self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).replace("\n\n", "\n")
        # å› ä¸ºæœ‰thinkï¼Œä¸èƒ½æŠŠthinkå¡«å…¥å†å²
        history.append((query, answer.split("</think>")[1].strip()))
        return history, answer

    def set_model_language(self, language="ä¸­æ–‡"):
        self.language = language
        # è¯»å–è§’è‰²ä¿¡æ¯
        with open(self.sys_prompt_dic[language], "r", encoding="utf-8") as f:
            self.system_prompt = "".join(f.readlines())


if __name__ == "__main__":
    """
    å¯¹å¤–è°ƒç”¨ç¤ºä¾‹
    """
    model = ChatModel()
    history = []
    while 1:
        model.set_model_language("ä¸­æ–‡")
        history, text = model.chat(history)
        if text.lower() == "exit":
            break
        print(text)
    print("history: \n", history)